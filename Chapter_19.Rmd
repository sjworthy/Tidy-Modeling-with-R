---
title: "Chapter_19"
author: "Sam Worthy"
date: "2024-04-28"
output: 
  html_document: 
    keep_md: yes
---

## When should you trust your predictions?

When a new data point in well outside of the range of data used to create the model, making a prediction may be an inappropriate extrapolation. A more qualitative example of an inappropriate prediction would be when the model is used in a completely different context.
For example, a model build on human breast cancer cells should not be applied to stomach cells. We can produce a prediction but it is unlikely to be applicable to the different cell type. This chapter covers two methods for quantifying the potential quality of a prediction:

* Equivocal zones use the predicted values to alert the user that results may be suspect.
* Applicability uses the predictors to measure the amount of extrapolation (if any) for new samples. 

### 19.1 Equivocal Results

The equivocal zone is the range of results in which the prediction should not be reported to patients, for example, some range of COVID-19 test results that are too uncertain to be reported to the patient. 

```{r}
library(tidymodels)
tidymodels_prefer()

simulate_two_classes <- 
  function (n, error = 0.1, eqn = quote(-1 - 2 * x - 0.2 * x^2 + 2 * y^2))  {
    # Slightly correlated predictors
    sigma <- matrix(c(1, 0.7, 0.7, 1), nrow = 2, ncol = 2)
    dat <- MASS::mvrnorm(n = n, mu = c(0, 0), Sigma = sigma)
    colnames(dat) <- c("x", "y")
    cls <- paste0("class_", 1:2)
    dat <- 
      as_tibble(dat) %>% 
      mutate(
        linear_pred = !!eqn,
        # Add some misclassification noise
        linear_pred = linear_pred + rnorm(n, sd = error),
        prob = binomial()$linkinv(linear_pred),
        class = ifelse(prob > runif(n), cls[1], cls[2]),
        class = factor(class, levels = cls)
      )
    dplyr::select(dat, x, y, class)
  }

set.seed(1901)
training_set <- simulate_two_classes(200)
testing_set  <- simulate_two_classes(50)
```

```{r}
two_class_mod <- 
  logistic_reg() %>% 
  set_engine("stan", seed = 1902) %>% 
  fit(class ~ . + I(x^2)+ I(y^2), data = training_set)
print(two_class_mod, digits = 3)
```

One simple method for disqualifying some results is to call them "equivocal" if the values are within some range around 50% (or the appropriate probability cutoff for a certain situation). Depending on the problem the model is being applied to, this might indicate we should collect another measurement or we require more information before a trustworthy prediction is possible.

We could base the width of the band around the cutoff on how performance improves when the uncertain results are removed. However, we should also estimate the reportable rate (the expected proportion of usable results). For example, it would not be useful in real-world situations to have perfect performance but release predictions on only 2% of the samples passed to the model.

```{r}
test_pred <- augment(two_class_mod, testing_set)
test_pred %>% head()
```

With tidymodels, the probably package contains functions for equivocal zones. For cases with two classes, the make_two_class_pred() function creates a factor-like column that has the predicted classes with an equivocal zone. 

```{r}
library(probably)

lvls <- levels(training_set$class)

test_pred <- 
  test_pred %>% 
  mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = 0.15))

test_pred %>% count(.pred_with_eqz)
```

Rows that are within 0.50 +- 0.15 are given a value of [EQ].

Since the factor levels are the same as the original data, confusion matrices and other statistics can be computed without error. When using standard functions from the yardstick package, the equivocal results are converted to NA and are not used in the calculations that use the hard class predictions.

```{r}
# All data
test_pred %>% conf_mat(class, .pred_class)
```

```{r}
# Reportable results only: 
test_pred %>% conf_mat(class, .pred_with_eqz)
```

An is_equivocal() function is available for filtering these rows from the data

```{r}
# A function to change the buffer then compute performance.
eq_zone_results <- function(buffer) {
  test_pred <- 
    test_pred %>% 
    mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = buffer))
  acc <- test_pred %>% accuracy(class, .pred_with_eqz)
  rep_rate <- reportable_rate(test_pred$.pred_with_eqz)
  tibble(accuracy = acc$.estimate, reportable = rep_rate, buffer = buffer)
}

# Evaluate a sequence of buffers and plot the results. 
map(seq(0, .1, length.out = 40), eq_zone_results) %>% 
  list_rbind() %>% 
  pivot_longer(c(-buffer), names_to = "statistic", values_to = "value") %>% 
  ggplot(aes(x = buffer, y = value, lty = statistic)) + 
  geom_step(linewidth = 1.2, alpha = 0.8) + 
  labs(y = NULL, lty = NULL)
```

Accuracy improves by a few percentage points but at the cost of nearly 10% of predictions being unusable. The value of such a compromise depends on how the model predictions will be used.

This analysis focused on using the predicted class probability to disqualify points, since this is a fundamental measure of uncertainty in classification models. A slightly better approache would be to use the standard error of the class probability. Since we used a Bayesian model, the probability estimates we found are actually the mean of the posterior predictive distribution. In other words, the Bayesian model gives us a distribution for the class probability. Measuring the standard deviation of this distribution gives us a standard error of prediction of the probability. In most cases, this value is directly related to the mean class probability. 

One important aspect of the standard error of prediction is that it takes into account more than just the class probability. In cases where there is significant extrapolation or aberrant predictor values, the standard error might increase. The benefit of using the standard error of prediction is that it might also flag predictions that are problematic (as opposed to simply uncertain). One reason we used the Bayesian model is that it naturally estimates the standard error of prediction; not many models can calculate this. For our test set, using type = "pred_int" will produce upper and lower limits and the std_error adds a column for that quantity. 

```{r}
test_pred <- 
  test_pred %>% 
  bind_cols(
    predict(two_class_mod, testing_set, type = "pred_int", std_error = TRUE)
  )
```

### 19.2 Determining model applicability

Equivocal zones try to measure the reliability of a prediction based on the model outputs. It may be that model statistics, such as the standard error of prediction, cannot measure the impact of extrapolation, and so we need another way to assess whether to trust a prediction and answer, "Is our model applicable for predicting a specific data point?"

The goal is to predict the number of customers entering the Clark and Lake train station each day.

```{r}
library(modeldata)

data(Chicago)

Chicago <- Chicago %>% select(ridership, date, one_of(stations))

n <- nrow(Chicago)

Chicago_train <- Chicago %>% slice(1:(n - 14))
Chicago_test  <- Chicago %>% slice((n - 13):n)
```

The main predictors are lagged ridership at different train stations as well as the date. The ridership predictors are highly correlated with on another. In the following recipe, the date column is expanded into several new features, and the ridership predictors are represented using partial least squares components. PLS is a supervised version of PCA where the new features have been decorrelated but are predictive of the outcome data.

```{r}
base_recipe <-
  recipe(ridership ~ ., data = Chicago_train) %>%
  # Create date features
  step_date(date) %>%
  step_holiday(date, keep_original_cols = FALSE) %>%
  # Create dummy variables from factor columns
  step_dummy(all_nominal()) %>%
  # Remove any columns with a single unique value
  step_zv(all_predictors()) %>%
  step_normalize(!!!stations)%>%
  step_pls(!!!stations, num_comp = 10, outcome = vars(ridership))

lm_spec <-
  linear_reg() %>%
  set_engine("lm") 

lm_wflow <-
  workflow() %>%
  add_recipe(base_recipe) %>%
  add_model(lm_spec)

set.seed(1902)
lm_fit <- fit(lm_wflow, data = Chicago_train)
```

How well do the data fit on the test set? We can predict() for the test set to find both predictions and prediction intervals.

```{r}
res_test <-
  predict(lm_fit, Chicago_test) %>%
  bind_cols(
    predict(lm_fit, Chicago_test, type = "pred_int"),
    Chicago_test
  )

res_test %>% select(date, ridership, starts_with(".pred"))

res_test %>% rmse(ridership, .pred)
```

These are fairly good results.

Given the scale of the ridership numbers, these results look particularly good for such a simple model. If this model were deployed how well would it have done a few years later in June 2020? The model successfully makes a prediction, as a predictive model almost alwasy will when given input data.

```{r}
load("Chicago_2020.RData")

res_2020 <-
  predict(lm_fit, Chicago_2020) %>%
  bind_cols(
    predict(lm_fit, Chicago_2020, type = "pred_int"),
    Chicago_2020
  ) 

res_2020 %>% select(date, contains(".pred"))
```

The prediction intervals are about the same width, even thoguh these data are well beyond the time period of the original training set. However, given the global pandemic in 2020, the performance of these data are abysmal:

```{r}
res_2020 %>% select(date, ridership, starts_with(".pred"))

res_2020 %>% rmse(ridership, .pred)

```

Confidence and prediction intervals for linear regression expand as the data become more and more removed from the center of the training set. However, that effect is not dramatic enough to flag these predictions as being poor.

This situation can be avoided by having a secondary methodology that can quantify how applicable the model is for any new prediction (i.e. the model's applicability domain). There are a variety of methods to compute an applicability domain model. The approach used in this chapter is a fairly simple unsupervised method that attempts to measure how much (if any) a new data point is beyond the training data.

The idea is to accompany a prediction with a socre that measures how similar the new point is to the training set. 

One method that works well uses PCA on the numeric predictor values. We'll illustrate the process by using only two of the predictors that correspond to ridership at different stations (California and Austin). 

The first step is to conduct PCA on the training data. Next using these results, we measure the distance of each training set point to the center of the PCA data. We then use this reference distribution to estimate how far a data point is from the mainstream of the training data. 

For a new sample, the PCA scores are computed along with the distance to the center of the training set. However, what does it mean when a new sample has a distance of X? Since the PCA components can have different ranges from data set to data set, there is no obvious limit to say that a distance is too large. 

One approach is to treat the distances from the training set data as "normal". For new samples, we can determine how the new distance compares to the range in the reference distribution (from the training set). A percentile can be computed for the new samples that reflect how much of the training set is less extreme than the new samples. 

A percentile of 90% means that most of the training set data are closer to the data center than the new sample. 

The applicable package can develop an applicability domain model using PCA. We'll use the 20 lagged station ridership predictors as inputs into the PCA analysis. There is an additional argument called threshold that determines how many components are used in the distance calculation. For our example, we'll use a large value that indicates we should use enough components to account for 99% of the variation in the ridership predictors.

```{r}
library(applicable)
pca_stat <- apd_pca(~ ., data = Chicago_train %>% select(one_of(stations)), 
                    threshold = 0.99)
pca_stat
```

The autoplot() method plots the reference distribution. It has an optional argument for which data to plot. We'll add a value of distance to plot only the training set distance distribution. 

```{r}
autoplot(pca_stat, distance) + labs(x = "distance")
```

The x-axis shows the values of the distance and the y-axis displays the distribution's percentiles. 

To compute the percentiles for new date, the score() function works in the same way as predict(). 

```{r}
score(pca_stat, Chicago_test) %>% select(starts_with("distance"))
```

These seem fairly reasonable. For the 2020 data:

```{r}
score(pca_stat, Chicago_2020) %>% select(starts_with("distance"))
```

The 2020 distance values indicate that these predictor values are outside of the vast majority of data seen by the model at training time. These should be flagged so that the predictions are either not reported at all or viewed with skepticism. 